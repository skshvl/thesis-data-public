{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function/variable declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_clustering.py:35: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_clustering.py:54: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_clustering.py:63: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window(order, start, length):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_clustering.py:69: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_clustering.py:77: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _mask_delta_score(m1, m2):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/links.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def identity(x):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/links.py:10: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _identity_inverse(x):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/links.py:15: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def logit(x):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/links.py:20: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _logit_inverse(x):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_masked_model.py:363: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_masked_model.py:385: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/maskers/_tabular.py:185: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/maskers/_tabular.py:196: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
     ]
    }
   ],
   "source": [
    "# conda activate shap (rampage)\n",
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os, copy, sys\n",
    "import math, json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# # Get the current script's directory\n",
    "# current_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# # Set the working directory to the current script's directory\n",
    "# os.chdir(current_directory)\n",
    "\n",
    "from shap_utils import load_valse_data, compute_mm_score, load_models\n",
    "\n",
    "\n",
    "def custom_masker_bimodal(mask, x):\n",
    "    \"\"\"\n",
    "    Shap relevant function. Defines the masking function so the shap computation\n",
    "    can 'know' how the model prediction looks like when some tokens are masked.\n",
    "    \"\"\"\n",
    "    masked_X = x.clone()\n",
    "    mask = torch.tensor(mask).unsqueeze(0)\n",
    "    masked_X[~mask] = 0  # ~mask !!! to zero\n",
    "    #\n",
    "    #  never mask out CLS and SEP tokens (makes no sense for the model to work without them)\n",
    "    masked_X[0, 0] = 49406\n",
    "    masked_X[0, text_length_tok-1] = 49407\n",
    "\n",
    "    # print(f\"Masking X in bimodal SHAP with text length {text_length_tok}. Masked X:\", masked_X)\n",
    "    \n",
    "    return masked_X\n",
    "\n",
    "def custom_masker_image_only_no_text_output(mask, x):\n",
    "    \"\"\"\n",
    "    Shap relevant function. Defines the masking function so the shap computation\n",
    "    can 'know' how the model prediction looks like when some tokens are masked.\n",
    "\n",
    "    mask is only the length of IMAGE!\n",
    "\n",
    "    NO TEXT TOKENS IN OUTPUT\n",
    "    \"\"\"\n",
    "    masked_X = x.clone()\n",
    "    mask = torch.tensor(mask).unsqueeze(0)\n",
    "    masked_X[~mask] = 0  # ~mask !!! to zero\n",
    "\n",
    "    # add UNMASKED TEXT tokens\n",
    "    masked_X = torch.tensor(masked_X)\n",
    "\n",
    "    #print(f\"Masking image in SHAP. Masked X:\", masked_X)\n",
    "    \n",
    "    return masked_X\n",
    "\n",
    "def get_clip_prediction_two_captions(x_stacked):\n",
    "    \"\"\"x is image masking map, get difference of two caption predictions (captions taken from inputs variable)\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        row_cols = 224 // patch_size # 224 / 32 = 7\n",
    "        result = np.zeros((x_stacked.shape[0]))\n",
    "        masked_image_token_ids = torch.tensor(x_stacked)\n",
    "        \n",
    "\n",
    "        for row in range(x_stacked.shape[0]):\n",
    "            masked_inputs = copy.deepcopy(inputs)\n",
    "            for token_position in range(masked_image_token_ids[row].shape[0]):\n",
    "                if masked_image_token_ids[row][token_position] == 0:  # should be zero\n",
    "                    m = token_position // row_cols\n",
    "                    n = token_position % row_cols\n",
    "                    masked_inputs[\"pixel_values\"][:, :, m *\n",
    "                        patch_size:(m+1)*patch_size, n*patch_size:(n+1)*patch_size] = 0 # torch.rand(3, patch_size, patch_size)  # np.random.rand()\n",
    "            \n",
    "                # \n",
    "            outputs = torch.tensor(clip_model(**masked_inputs).logits_per_text)\n",
    "\n",
    "            # based on global variable CLIP_RETURN, we can choose whether it returns the difference, caption or foil score\n",
    "            if CLIP_RETURN == \"diff\":\n",
    "                result[row] = outputs[0,0]-outputs[1,0]\n",
    "            elif CLIP_RETURN == \"caption\":\n",
    "                result[row] = outputs[0,0]\n",
    "            elif CLIP_RETURN == \"foil\":\n",
    "                result[row] = outputs[1,0]\n",
    "            else:\n",
    "                print(\"NO RETURN INSTRUCTION TO CLIP. ABORTING\")\n",
    "                sys.exit()\n",
    "            # print(f\"Mask: {masked_image_token_ids[row]}\\n Outputs: {outputs}, difference: {result[row]}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_clip_prediction_one_caption(x):\n",
    "    \"\"\"\n",
    "    Shap relevant function. Predict the model output for all combinations of masked tokens.\n",
    "    EDUARD: This does the following:\n",
    "    - Copy the masked input mapping (paramter x) into text and image parts (originally it's a concatenation)\n",
    "    - Use the image of x to part to generate a new image based on the original input variable (global)\n",
    "    - Copy x's masked text (input_ids) and the masked image both into masked_inputs variable\n",
    "    - Use the masked_inputs variable (consisting of input_ids and pixel_values, both masked) to make a model prediction and get the logit result\n",
    "    - Theoretically it works for multiple captions at once, but in practice we don't do it that way.\n",
    "    - All this is with x being the OUTPUT of a shapley being done to the X input\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        input_ids = torch.tensor(x[:, :inputs.input_ids.shape[1]])\n",
    "        masked_image_token_ids = torch.tensor(x[:, inputs.input_ids.shape[1]:])\n",
    "        result = np.zeros(input_ids.shape[0])\n",
    "        row_cols = 224 // patch_size # 224 / 32 = 7\n",
    "        for i in range(input_ids.shape[0]): # loop through x instances, which can be MULTIPLE\n",
    "\n",
    "            masked_inputs = copy.deepcopy(inputs)  # initialize the masked inputs variable which will be modified version of current inputs\n",
    "            masked_inputs['input_ids'] = input_ids[i].unsqueeze(0) # this is the text part of the x data (already masked)\n",
    "            for k in range(masked_image_token_ids[i].shape[0]): # loop through token indexes\n",
    "                if masked_image_token_ids[i][k] == 0:  # if zero, mask region corresponding to token k\n",
    "                    m = k // row_cols\n",
    "                    n = k % row_cols\n",
    "                    masked_inputs[\"pixel_values\"][:, :, m *\n",
    "                        patch_size:(m+1)*patch_size, n*patch_size:(n+1)*patch_size] = 0 # torch.rand(3, patch_size, patch_size)  # np.random.rand()\n",
    "            \n",
    "            # \n",
    "            outputs = clip_model(**masked_inputs)\n",
    "            result[i] = outputs.logits_per_image\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered VALSE data to only given filenames/caption/foil trios (179), down to 179 images. Among them 179 unique filenames\n",
      "Removed any duplicates of image + caption + foil. Now we have {len(df)} total stimuli\n",
      "Filtered VALSE data to only ['existence', 'relations', 'actions'], down to 179 images.\n",
      "Data loaded with sampling: 179 rows\n"
     ]
    }
   ],
   "source": [
    "clip_model, clip_processor = load_models(\"clip\")\n",
    "\n",
    "# load VALSE data\n",
    "\n",
    "# get stimuli table and generate list of filenames/captions\n",
    "STIMULI_PATH = \"../data_prep_and_analysis/jan8_preliminary_sampled_stimuli.pickle\"\n",
    "stimuli_df = pd.read_pickle(STIMULI_PATH)\n",
    "# make list of tuples: image filename, caption\n",
    "filename_caption_foil_trios = [(os.path.basename(img_path), caption, foil) for img_path, caption, foil in zip(stimuli_df['img_path'], stimuli_df['caption'], stimuli_df['foil'])]\n",
    "\n",
    "# load VALSE data for only those filenames\n",
    "data_list = load_valse_data(n_samples = \"all\", filename_caption_foil_trios = filename_caption_foil_trios,\n",
    "                            ling_phenomena = [\"existence\", \"relations\", \"actions\"]) # ling phenomena IGNORED if filenames given\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>caption</th>\n",
       "      <th>foil</th>\n",
       "      <th>linguistic_phenomena</th>\n",
       "      <th>clip_pred_caption</th>\n",
       "      <th>clip_pred_foil</th>\n",
       "      <th>clip_pred_diff</th>\n",
       "      <th>performance_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>../VALSE_data/images/swig/stretching_291.jpg</td>\n",
       "      <td>A person stretches their neck.</td>\n",
       "      <td>A person checks their neck.</td>\n",
       "      <td>actions</td>\n",
       "      <td>32.015369</td>\n",
       "      <td>29.466202</td>\n",
       "      <td>2.549168</td>\n",
       "      <td>high_perf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          img_path  \\\n",
       "1183  ../VALSE_data/images/swig/stretching_291.jpg   \n",
       "\n",
       "                             caption                         foil  \\\n",
       "1183  A person stretches their neck.  A person checks their neck.   \n",
       "\n",
       "     linguistic_phenomena  clip_pred_caption  clip_pred_foil  clip_pred_diff  \\\n",
       "1183              actions          32.015369       29.466202        2.549168   \n",
       "\n",
       "     performance_group  \n",
       "1183         high_perf  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimuli_df[stimuli_df.apply(lambda row: os.path.basename(row['img_path']) == \"stretching_291.jpg\", axis = 1)] # this was the duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/179 [00:00<?, ?it/s]To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Permutation explainer: 2it [00:22, 22.86s/it]               \n",
      "Permutation explainer: 2it [00:22, 22.54s/it]               \n",
      "Permutation explainer: 2it [00:22, 22.76s/it]               \n",
      "Permutation explainer: 2it [00:23, 23.33s/it]               \n",
      "Permutation explainer: 2it [00:23, 23.20s/it]               \n",
      "Permutation explainer: 2it [00:23, 23.17s/it]               \n",
      "Permutation explainer: 2it [00:24, 24.04s/it]               \n",
      "Permutation explainer: 2it [00:23, 23.89s/it]               \n",
      "  1%|          | 2/179 [03:29<5:09:31, 104.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m clip_shap_caption \u001b[39m=\u001b[39m clip_explainer(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m CLIP_RETURN \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfoil\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m clip_shap_foil \u001b[39m=\u001b[39m clip_explainer(X)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m results[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclip_shap_diff\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(clip_shap_diff\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m results[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclip_shap_caption\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mappend(clip_shap_caption\u001b[39m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/explainers/_permutation.py:60\u001b[0m, in \u001b[0;36mPermutation.__init__.<locals>.Permutation.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_evals\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m              outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[1;32m     61\u001b[0m         \u001b[39m*\u001b[39;49margs, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds,\n\u001b[1;32m     62\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent\n\u001b[1;32m     63\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/explainers/_permutation.py:74\u001b[0m, in \u001b[0;36mPermutation.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_evals\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     71\u001b[0m              outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     72\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[1;32m     75\u001b[0m         \u001b[39m*\u001b[39;49margs, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     76\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/explainers/_explainer.py:258\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     feature_names \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(args))]\n\u001b[1;32m    257\u001b[0m \u001b[39mfor\u001b[39;00m row_args \u001b[39min\u001b[39;00m show_progress(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39margs), num_rows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m explainer\u001b[39m\u001b[39m\"\u001b[39m, silent):\n\u001b[0;32m--> 258\u001b[0m     row_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain_row(\n\u001b[1;32m    259\u001b[0m         \u001b[39m*\u001b[39;49mrow_args, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds,\n\u001b[1;32m    260\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    261\u001b[0m     )\n\u001b[1;32m    262\u001b[0m     values\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    263\u001b[0m     output_indices\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39moutput_indices\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/explainers/_permutation.py:134\u001b[0m, in \u001b[0;36mPermutation.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m    131\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[39m# evaluate the masked model\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m outputs \u001b[39m=\u001b[39m fm(masks, zero_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m row_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     row_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(fm),) \u001b[39m+\u001b[39m outputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:])\n",
      "File \u001b[0;32m~/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_masked_model.py:65\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m         full_masks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msum(masks \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_masker_cols), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m     64\u001b[0m         _convert_delta_mask_to_full(masks, full_masks)\n\u001b[0;32m---> 65\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_full_masking_call(full_masks, zero_index\u001b[39m=\u001b[39;49mzero_index, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_masking_call(masks, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/utils/_masked_model.py:141\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    138\u001b[0m         all_masked_inputs[i]\u001b[39m.\u001b[39mappend(copy\u001b[39m.\u001b[39mcopy(v))\n\u001b[1;32m    140\u001b[0m joined_masked_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stack_inputs(\u001b[39m*\u001b[39mall_masked_inputs)\n\u001b[0;32m--> 141\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49mjoined_masked_inputs)\n\u001b[1;32m    142\u001b[0m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[1;32m    143\u001b[0m all_outputs\u001b[39m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m~/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/shap/models/_model.py:21\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_model(\u001b[39m*\u001b[39;49margs))\n",
      "\u001b[1;32m/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         masked_inputs[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m][:, :, m \u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m             patch_size:(m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mpatch_size, n\u001b[39m*\u001b[39mpatch_size:(n\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mpatch_size] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# torch.rand(3, patch_size, patch_size)  # np.random.rand()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     \u001b[39m# \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(clip_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmasked_inputs)\u001b[39m.\u001b[39mlogits_per_text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# based on global variable CLIP_RETURN, we can choose whether it returns the difference, caption or foil score\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eduardsaakashvili/Desktop/UU/Thesis/code/es-thesis-repo/mm_shap_files/generate_clip_shap.ipynb#W6sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39mif\u001b[39;00m CLIP_RETURN \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiff\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:1131\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1122\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1124\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvision_model(\n\u001b[1;32m   1125\u001b[0m     pixel_values\u001b[39m=\u001b[39mpixel_values,\n\u001b[1;32m   1126\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m   1127\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1128\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1129\u001b[0m )\n\u001b[0;32m-> 1131\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_model(\n\u001b[1;32m   1132\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1133\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1134\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1135\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1136\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1137\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1138\u001b[0m )\n\u001b[1;32m   1140\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1141\u001b[0m image_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisual_projection(image_embeds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:740\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    737\u001b[0m     \u001b[39m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     attention_mask \u001b[39m=\u001b[39m _expand_mask(attention_mask, hidden_states\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 740\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    741\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    742\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    743\u001b[0m     causal_attention_mask\u001b[39m=\u001b[39;49mcausal_attention_mask,\n\u001b[1;32m    744\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    745\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    746\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    747\u001b[0m )\n\u001b[1;32m    749\u001b[0m last_hidden_state \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    750\u001b[0m last_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm(last_hidden_state)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:654\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    647\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    648\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    649\u001b[0m         hidden_states,\n\u001b[1;32m    650\u001b[0m         attention_mask,\n\u001b[1;32m    651\u001b[0m         causal_attention_mask,\n\u001b[1;32m    652\u001b[0m     )\n\u001b[1;32m    653\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    655\u001b[0m         hidden_states,\n\u001b[1;32m    656\u001b[0m         attention_mask,\n\u001b[1;32m    657\u001b[0m         causal_attention_mask,\n\u001b[1;32m    658\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    659\u001b[0m     )\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    663\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:393\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    391\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    392\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[0;32m--> 393\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    394\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    396\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:350\u001b[0m, in \u001b[0;36mCLIPMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    348\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m    349\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn(hidden_states)\n\u001b[0;32m--> 350\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(hidden_states)\n\u001b[1;32m    351\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = \"clip\"\n",
    "UNIMODAL_SHAP = True # whether to do unimodal shap, only masking images\n",
    "MULTIMODAL_SHAP = False # whether to calculate multimodality scores and use bimodal masking to do it\n",
    "\n",
    "# we output results of MMSHAP evaluation into a dictionary first, then into a dataframe\n",
    "results = defaultdict(list)\n",
    "\n",
    "\n",
    "for data_point in tqdm(data_list):\n",
    "\n",
    "    # copy over those things that should be copied directly\n",
    "    for name in [\"img_path\", \"caption\", \"foil\", \"linguistic_phenomena\"]:\n",
    "        results[name].append(data_point[name])\n",
    "\n",
    "    # prepare data point as inputs to test model on\n",
    "    image = Image.open(data_point[\"img_path\"])\n",
    "\n",
    "    test_sentences = [data_point[\"caption\"],\n",
    "                      data_point[\"foil\"]]\n",
    "    \n",
    "    # create CLIP input with BOTH captions\n",
    "    try:\n",
    "        inputs = clip_processor(\n",
    "            text = test_sentences,\n",
    "            images = image,\n",
    "            return_tensors = 'pt',\n",
    "            padding = True\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        print(f\"Trouble processing {data_point['img_path']}\")\n",
    "        continue\n",
    "    \n",
    "    clip_logits = clip_model(**inputs).logits_per_image[0,0:2].tolist()\n",
    "    clip_cap_foil_diff = clip_logits[0] - clip_logits[1]\n",
    "\n",
    "    # save relevant fields to the dictionary to either {caption} or {foil} (based on which it is)\n",
    "    results[\"clip_pred_caption\"].append(clip_logits[0]) # logit\n",
    "    results[\"clip_pred_foil\"].append(clip_logits[1])\n",
    "    results[\"clip_pred_diff\"].append(clip_cap_foil_diff)\n",
    "\n",
    "    if UNIMODAL_SHAP:\n",
    "        # Set up SHAP\n",
    "        NR_ROWS, patch_size = 4, 224 // 4 # patch size should divide by same number as NR_ROWS\n",
    "        image_token_ids = torch.tensor( range(1, NR_ROWS**2+1)).unsqueeze(0) #p x p patches        \n",
    "        X = image_token_ids.unsqueeze(1) # only image token ids are passed to the explainer\n",
    "\n",
    "        clip_explainer = shap.Explainer(\n",
    "                get_clip_prediction_two_captions, custom_masker_image_only_no_text_output, silent=False)\n",
    "        \n",
    "        # do SHAP explanations with different values of CLIP return\n",
    "        # This variable determines if clip output is score for caption, foil, or difference\n",
    "        CLIP_RETURN = \"diff\"\n",
    "        clip_shap_diff = clip_explainer(X)\n",
    "        CLIP_RETURN = \"caption\"\n",
    "        clip_shap_caption = clip_explainer(X)\n",
    "        CLIP_RETURN = \"foil\"\n",
    "        clip_shap_foil = clip_explainer(X)\n",
    "\n",
    "\n",
    "        results[f\"clip_shap_diff\"].append(clip_shap_diff.values)\n",
    "        results[f\"clip_shap_caption\"].append(clip_shap_caption.values)\n",
    "        results[f\"clip_shap_foil\"].append(clip_shap_foil.values)\n",
    "\n",
    "    # MULTIMODALITY calculation - pass each caption-img pair separately and run bimodal SHAP\n",
    "    if MULTIMODAL_SHAP:\n",
    "        # create new inputs variable for this sentence-img pair\n",
    "        for k, sentence in enumerate(test_sentences):\n",
    "            try:  # image feature extraction can go wrong\n",
    "                inputs = clip_processor(\n",
    "                    text=sentence, images=image, return_tensors=\"pt\", padding=True\n",
    "                )\n",
    "            except:\n",
    "                print(f\"Trouble processing {data_point['img_path']}\")\n",
    "                continue\n",
    "\n",
    "            text_length_tok = inputs.input_ids.shape[1] # nr of text tokens\n",
    "\n",
    "            p = int(math.ceil(np.sqrt(text_length_tok)))\n",
    "            patch_size = 224 // p # determine patch nr and patch size based on nr of tokens assuming 224x224 image\n",
    "            image_token_ids = torch.tensor( \n",
    "                range(1, p**2+1)).unsqueeze(0) #p x p patches        \n",
    "            # NOTE: There is no image data in X;The actual image pixel data is accessed by get_clip_prediction() from the inputs variable\n",
    "            X = torch.cat(\n",
    "                (inputs.input_ids, image_token_ids), 1).unsqueeze(1)\n",
    "\n",
    "            # create an explainer with model and image masker\n",
    "            explainer2 = shap.Explainer(\n",
    "                get_clip_prediction_one_caption, custom_masker_bimodal, silent=True)\n",
    "            shap_values = explainer2(X)\n",
    "\n",
    "            mm_score = compute_mm_score(text_length_tok, shap_values)\n",
    "\n",
    "            # if it's CAPTION (so k=0), update the data for caption        \n",
    "            if k == 0:\n",
    "                which = 'caption'\n",
    "            # if it's FOIL (so k=1), update the data for foil\n",
    "            else:\n",
    "                which = 'foil'\n",
    "\n",
    "            # save relevant fields to the dictionary to either {caption} or {foil} (based on which it is)\n",
    "            results[f\"mm_score_\"+which].append(mm_score) #multimodality score\n",
    "            results[f\"bimodal_shap_values_\"+which].append(shap_values.values) # shap values array\n",
    "            results[f\"text_tok_\"+which].append(text_length_tok) # number of text tokens\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_pickle(\"../data_prep_and_analysis/clip_all_results_jan11.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
