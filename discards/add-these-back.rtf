{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red15\green112\blue1;\red255\green255\blue255;\red45\green45\blue45;
\red157\green0\blue210;\red32\green108\blue135;\red0\green0\blue109;\red0\green0\blue0;\red101\green76\blue29;
\red0\green0\blue255;\red11\green90\blue180;\red144\green1\blue18;\red19\green118\blue70;\red230\green0\blue6;
}
{\*\expandedcolortbl;;\cssrgb\c0\c50196\c0;\cssrgb\c100000\c100000\c100000;\cssrgb\c23137\c23137\c23137;
\cssrgb\c68627\c0\c85882;\cssrgb\c14902\c49804\c60000;\cssrgb\c0\c6275\c50196;\cssrgb\c0\c0\c0;\cssrgb\c47451\c36863\c14902;
\cssrgb\c0\c0\c100000;\cssrgb\c0\c43922\c75686;\cssrgb\c63922\c8235\c8235;\cssrgb\c3529\c52549\c34510;\cssrgb\c93333\c0\c0;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 # conda activate shap (rampage)\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 shap\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 torch\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 numpy\cf4 \strokec4  \cf5 \strokec5 as\cf4 \strokec4  \cf6 \strokec6 np\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 from\cf4 \strokec4  \cf6 \strokec6 PIL\cf4 \strokec4  \cf5 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 Image\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 os\cf4 \strokec4 , \cf6 \strokec6 copy\cf4 \strokec4 , \cf6 \strokec6 sys\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 math\cf4 \strokec4 , \cf6 \strokec6 json\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 random\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 from\cf4 \strokec4  \cf6 \strokec6 tqdm\cf4 \strokec4  \cf5 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 tqdm\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 pickle\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 json\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 pandas\cf4 \strokec4  \cf5 \strokec5 as\cf4 \strokec4  \cf6 \strokec6 pd\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 from\cf4 \strokec4  \cf6 \strokec6 transformers\cf4 \strokec4  \cf5 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 CLIPProcessor\cf4 \strokec4 , \cf6 \strokec6 CLIPModel\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 import\cf4 \strokec4  \cf6 \strokec6 os\cf4 \cb1 \strokec4 \
\
\cf2 \cb3 \strokec2 # Get the current script's directory\cf4 \cb1 \strokec4 \
\cf7 \cb3 \strokec7 current_directory\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 os\cf4 \strokec4 .\cf7 \strokec7 path\cf4 \strokec4 .\cf9 \strokec9 dirname\cf4 \strokec4 (\cf6 \strokec6 os\cf4 \strokec4 .\cf7 \strokec7 path\cf4 \strokec4 .\cf9 \strokec9 abspath\cf4 \strokec4 (\cf7 \strokec7 __file__\cf4 \strokec4 ))\cb1 \
\
\cf2 \cb3 \strokec2 # Set the working directory to the current script's directory\cf4 \cb1 \strokec4 \
\cf6 \cb3 \strokec6 os\cf4 \strokec4 .\cf9 \strokec9 chdir\cf4 \strokec4 (\cf7 \strokec7 current_directory\cf4 \strokec4 )\cb1 \
\
\
\cf2 \cb3 \strokec2 # \cf10 \strokec10 NOTE\cf2 \strokec2 : This file has been heavily modified from MMSHAP repository to work SPECIFICALLY \cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 # for VALSE data as stored in VALSE_data/valse_challenges_with_img_paths.csv\cf4 \cb1 \strokec4 \
\
\cf2 \cb3 \strokec2 # Original version of this code can be found at: https://github.com/Heidelberg-NLP/MM-SHAP/blob/main/mm-shap_clip_dataset.py\cf4 \cb1 \strokec4 \
\
\cf2 \cb3 \strokec2 # POSSIBLE LINGUISTIC PHENOMENA:\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 # \{'actions',\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #  'coreference',\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #  'counting',\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #  'existence',\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #  'noun phrases',\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #  'plurals',\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #  'relations'\}\cf4 \cb1 \strokec4 \
\
\cf11 \cb3 \strokec11 VALSE_DATA_PATH\cf4 \cb3 \strokec4  \strokec8 =\strokec4  \cf12 \strokec12 "../VALSE_data/valse_challenges_with_img_paths.csv"\cf4 \cb1 \strokec4 \
\
\cf2 \cb3 \strokec2 # EDUARD: I wrote the function below specifically for my thesis\cf4 \cb1 \strokec4 \
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 load_valse_data\cf4 \strokec4 (\cf7 \strokec7 n_samples\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 2\cf4 \strokec4 , \cf7 \strokec7 ling_phenomenon\cf4 \strokec8 =\cf10 \strokec10 None\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """Return: List of dictionaries with elements "img_path", "caption", "foil", "linguistic_phenomena"\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     """\cf4 \cb1 \strokec4 \
\
\cb3     \cf7 \strokec7 df\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 pd\cf4 \strokec4 .\cf9 \strokec9 read_csv\cf4 \strokec4 (\cf11 \cb3 \strokec11 VALSE_DATA_PATH\cf4 \cb3 \strokec4 , \cf7 \strokec7 converters\cf4 \strokec8 =\strokec4 \{\cf12 \strokec12 "mturk"\cf4 \strokec4 : \cf9 \strokec9 eval\cf4 \strokec4 \}) \cf2 \strokec2 # evaluate mturk column as actual dictionaries\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 # only accept images accepted by MTURK annotators (need to load string version of each row's dict into dict thru json.loads)\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 df\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 df\cf4 \strokec4 [\cf7 \strokec7 df\cf4 \strokec4 [\cf12 \strokec12 'mturk'\cf4 \strokec4 ].\cf9 \strokec9 apply\cf4 \strokec4 (\cf10 \strokec10 lambda\cf4 \strokec4  \cf7 \strokec7 x\cf4 \strokec4 : \cf7 \strokec7 x\cf4 \strokec4 [\cf12 \strokec12 'caption'\cf4 \strokec4 ] \strokec8 >=\strokec4  \cf13 \strokec13 2\cf4 \strokec4 )]\cb1 \
\
\cb3     \cf2 \strokec2 # filter by ling phenomenon if necessary\cf4 \cb1 \strokec4 \
\cb3     \cf5 \strokec5 if\cf4 \strokec4  \cf7 \strokec7 ling_phenomenon\cf4 \strokec4 :\cb1 \
\cb3         \cf7 \strokec7 df\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 df\cf4 \strokec4 [\cf7 \strokec7 df\cf4 \strokec4 [\cf12 \strokec12 "linguistic_phenomena"\cf4 \strokec4 ]\cf9 \strokec9 ==\cf7 \strokec7 ling_phenomenon\cf4 \strokec4 ]\cb1 \
\cb3         \cf9 \strokec9 print\cf4 \strokec4 (\cf10 \strokec10 f\cf12 \strokec12 "Filtered VALSE data to only \cf10 \strokec10 \{\cf7 \strokec7 ling_phenomenon\cf10 \strokec10 \}\cf12 \strokec12 , down to \cf10 \strokec10 \{\cf9 \strokec9 len\cf4 \strokec4 (\cf7 \strokec7 df\cf4 \strokec4 )\cf10 \strokec10 \}\cf12 \strokec12  images."\cf4 \strokec4 )\cb1 \
\
\cb3     \cf2 \strokec2 # sample n_samples rows\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 df\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 df\cf4 \strokec4 .\cf9 \strokec9 sample\cf4 \strokec4 (\cf7 \strokec7 n_samples\cf4 \strokec4 , \cf7 \strokec7 random_state\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 42\cf4 \strokec4 )\cb1 \
\
\cb3     \cf7 \strokec7 output_data\cf4 \strokec4  \strokec8 =\strokec4  []\cb1 \
\
\cb3     \cf5 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 index\cf4 \strokec4 , \cf7 \strokec7 row\cf4 \strokec4  \cf5 \strokec5 in\cf4 \strokec4  \cf7 \strokec7 df\cf4 \strokec4 .\cf9 \strokec9 iterrows\cf4 \strokec4 ():\cb1 \
\cb3         \cf7 \strokec7 output_data\cf4 \strokec4 .\cf9 \strokec9 append\cf4 \strokec4 (\cb1 \
\cb3             \{\cb1 \
\cb3                 \cf2 \strokec2 # add image path (add VALSE_data prefix because image paths are relative to VALSE_data folder)\cf4 \cb1 \strokec4 \
\cb3                 \cf12 \strokec12 "img_path"\cf4 \strokec4 :\cf6 \strokec6 os\cf4 \strokec4 .\cf7 \strokec7 path\cf4 \strokec4 .\cf9 \strokec9 join\cf4 \strokec4 (\cf12 \strokec12 "../VALSE_data"\cf4 \strokec4 , \cf7 \strokec7 row\cf4 \strokec4 [\cf12 \strokec12 "local_img_path"\cf4 \strokec4 ]),\cb1 \
\
\cb3                 \cf2 \strokec2 # caption and foil\cf4 \cb1 \strokec4 \
\cb3                 \cf12 \strokec12 "caption"\cf4 \strokec4 :\cf7 \strokec7 row\cf4 \strokec4 [\cf12 \strokec12 "caption"\cf4 \strokec4 ],\cb1 \
\cb3                 \cf12 \strokec12 "foil"\cf4 \strokec4 :\cf7 \strokec7 row\cf4 \strokec4 [\cf12 \strokec12 "foil"\cf4 \strokec4 ],\cb1 \
\
\cb3                 \cf12 \strokec12 "linguistic_phenomena"\cf4 \strokec4 :\cf7 \strokec7 row\cf4 \strokec4 [\cf12 \strokec12 "linguistic_phenomena"\cf4 \strokec4 ]\cb1 \
\
\
\cb3             \}\cb1 \
\cb3         )\cb1 \
\
\cb3     \cf9 \strokec9 print\cf4 \strokec4 (\cf10 \strokec10 f\cf12 \strokec12 "Data loaded with sampling: \cf10 \strokec10 \{\cf9 \strokec9 len\cf4 \strokec4 (\cf7 \strokec7 output_data\cf4 \strokec4 )\cf10 \strokec10 \}\cf12 \strokec12  rows"\cf4 \strokec4 )\cb1 \
\cb3     \cb1 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 output_data\cf4 \cb1 \strokec4 \
\
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 custom_masker_bimodal\cf4 \strokec4 (\cf7 \strokec7 mask\cf4 \strokec4 , \cf7 \strokec7 x\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     Shap relevant function. Defines the masking function so the shap computation\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     can 'know' how the model prediction looks like when some tokens are masked.\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     """\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 x\cf4 \strokec4 .clone()\cb1 \
\cb3     \cf7 \strokec7 mask\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 mask\cf4 \strokec4 ).\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 )\cb1 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4 [\cf9 \strokec9 ~\cf7 \strokec7 mask\cf4 \strokec4 ] \strokec8 =\strokec4  \cf13 \strokec13 0\cf4 \strokec4   \cf2 \strokec2 # ~mask !!! to zero\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 #\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 #  never mask out CLS and SEP tokens (makes no sense for the model to work without them)\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4 [\cf13 \strokec13 0\cf4 \strokec4 , \cf13 \strokec13 0\cf4 \strokec4 ] \strokec8 =\strokec4  \cf13 \strokec13 49406\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4 [\cf13 \strokec13 0\cf4 \strokec4 , \cf7 \strokec7 text_length_tok\cf4 \strokec8 -\cf13 \strokec13 1\cf4 \strokec4 ] \strokec8 =\strokec4  \cf13 \strokec13 49407\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 # print(f"Masking X in bimodal SHAP with text length \{text_length_tok\}. Masked X:", masked_X)\cf4 \cb1 \strokec4 \
\cb3     \cb1 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 masked_X\cf4 \cb1 \strokec4 \
\
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 custom_masker_image_only\cf4 \strokec4 (\cf7 \strokec7 mask\cf4 \strokec4 , \cf7 \strokec7 x\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     Shap relevant function. Defines the masking function so the shap computation\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     can 'know' how the model prediction looks like when some tokens are masked.\cf4 \cb1 \strokec4 \
\
\cf12 \cb3 \strokec12     mask is only the length of IMAGE!\cf4 \cb1 \strokec4 \
\
\cf12 \cb3 \strokec12     Unmasked text tokens added at the beginning for model to interpret\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     """\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 x\cf4 \strokec4 .clone()\cb1 \
\cb3     \cf7 \strokec7 mask\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 mask\cf4 \strokec4 ).\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 )\cb1 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4 [\cf9 \strokec9 ~\cf7 \strokec7 mask\cf4 \strokec4 ] \strokec8 =\strokec4  \cf13 \strokec13 0\cf4 \strokec4   \cf2 \strokec2 # ~mask !!! to zero\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 # add UNMASKED TEXT tokens\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 cat\cf4 \strokec4 (\cb1 \
\cb3             (\cf7 \strokec7 inputs\cf4 \strokec4 .input_ids, \cf7 \strokec7 masked_X\cf4 \strokec4 ), \cf13 \strokec13 1\cf4 \strokec4 )\cb1 \
\
\cb3     \cf2 \strokec2 # print(f"Masking X in unimodal SHAP (text unaffected). Masked X:", masked_X)\cf4 \cb1 \strokec4 \
\cb3     \cb1 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 masked_X\cf4 \cb1 \strokec4 \
\
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 custom_masker_image_only_no_text_output\cf4 \strokec4 (\cf7 \strokec7 mask\cf4 \strokec4 , \cf7 \strokec7 x\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     Shap relevant function. Defines the masking function so the shap computation\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     can 'know' how the model prediction looks like when some tokens are masked.\cf4 \cb1 \strokec4 \
\
\cf12 \cb3 \strokec12     mask is only the length of IMAGE!\cf4 \cb1 \strokec4 \
\
\cf12 \cb3 \strokec12     NO TEXT TOKENS IN OUTPUT\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     """\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 x\cf4 \strokec4 .clone()\cb1 \
\cb3     \cf7 \strokec7 mask\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 mask\cf4 \strokec4 ).\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 )\cb1 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4 [\cf9 \strokec9 ~\cf7 \strokec7 mask\cf4 \strokec4 ] \strokec8 =\strokec4  \cf13 \strokec13 0\cf4 \strokec4   \cf2 \strokec2 # ~mask !!! to zero\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 # add UNMASKED TEXT tokens\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 masked_X\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 masked_X\cf4 \strokec4 )\cb1 \
\
\cb3     \cf9 \strokec9 print\cf4 \strokec4 (\cf10 \strokec10 f\cf12 \strokec12 "Masking image in SHAP. Masked X:"\cf4 \strokec4 , \cf7 \strokec7 masked_X\cf4 \strokec4 )\cb1 \
\cb3     \cb1 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 masked_X\cf4 \cb1 \strokec4 \
\
\
\
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 get_clip_prediction_diff\cf4 \strokec4 (\cf7 \strokec7 x_stacked\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """x is image masking map, get difference of two caption predictions (captions taken from inputs variable)"""\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 #return [0.0,0.0]\cf4 \cb1 \strokec4 \
\cb3     \cb1 \
\cb3     \cf5 \strokec5 with\cf4 \strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf6 \strokec6 no_grad\cf4 \strokec4 ():\cb1 \
\cb3         \cf7 \strokec7 row_cols\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 224\cf4 \strokec4  \strokec8 //\strokec4  \cf7 \strokec7 patch_size\cf4 \strokec4  \cf2 \strokec2 # 224 / 32 = 7\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 result\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 np\cf4 \strokec4 .\cf9 \strokec9 zeros\cf4 \strokec4 ((\cf7 \strokec7 x_stacked\cf4 \strokec4 .shape[\cf13 \strokec13 0\cf4 \strokec4 ],\cf13 \strokec13 2\cf4 \strokec4 ))\cb1 \
\cb3         \cf7 \strokec7 masked_image_token_ids\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 x_stacked\cf4 \strokec4 )\cb1 \
\cb3         \cf7 \strokec7 masked_inputs\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 copy\cf4 \strokec4 .\cf9 \strokec9 deepcopy\cf4 \strokec4 (\cf7 \strokec7 inputs\cf4 \strokec4 )\cb1 \
\
\cb3         \cf5 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 row\cf4 \strokec4  \cf5 \strokec5 in\cf4 \strokec4  \cf6 \strokec6 range\cf4 \strokec4 (\cf7 \strokec7 x_stacked\cf4 \strokec4 .shape[\cf13 \strokec13 0\cf4 \strokec4 ]):\cb1 \
\cb3             \cf5 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 token_position\cf4 \strokec4  \cf5 \strokec5 in\cf4 \strokec4  \cf6 \strokec6 range\cf4 \strokec4 (\cf7 \strokec7 masked_image_token_ids\cf4 \strokec4 [\cf7 \strokec7 row\cf4 \strokec4 ].\cf7 \strokec7 shape\cf4 \strokec4 [\cf13 \strokec13 0\cf4 \strokec4 ]):\cb1 \
\cb3                 \cf5 \strokec5 if\cf4 \strokec4  \cf7 \strokec7 masked_image_token_ids\cf4 \strokec4 [\cf7 \strokec7 row\cf4 \strokec4 ][\cf7 \strokec7 token_position\cf4 \strokec4 ] \cf9 \strokec9 ==\cf4 \strokec4  \cf13 \strokec13 0\cf4 \strokec4 :  \cf2 \strokec2 # should be zero\cf4 \cb1 \strokec4 \
\cb3                     \cf7 \strokec7 m\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 token_position\cf4 \strokec4  \strokec8 //\strokec4  \cf7 \strokec7 row_cols\cf4 \cb1 \strokec4 \
\cb3                     \cf7 \strokec7 n\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 token_position\cf4 \strokec4  \strokec8 %\strokec4  \cf7 \strokec7 row_cols\cf4 \cb1 \strokec4 \
\cb3                     \cf7 \strokec7 masked_inputs\cf4 \strokec4 [\cf12 \strokec12 "pixel_values"\cf4 \strokec4 ][:, :, \cf7 \strokec7 m\cf4 \strokec4  \strokec8 *\cb1 \strokec4 \
\cb3                         \cf7 \strokec7 patch_size\cf4 \strokec4 :(\cf7 \strokec7 m\cf4 \strokec8 +\cf13 \strokec13 1\cf4 \strokec4 )\strokec8 *\cf7 \strokec7 patch_size\cf4 \strokec4 , \cf7 \strokec7 n\cf4 \strokec8 *\cf7 \strokec7 patch_size\cf4 \strokec4 :(\cf7 \strokec7 n\cf4 \strokec8 +\cf13 \strokec13 1\cf4 \strokec4 )\strokec8 *\cf7 \strokec7 patch_size\cf4 \strokec4 ] \strokec8 =\strokec4  \cf13 \strokec13 0\cf4 \strokec4  \cf2 \strokec2 # torch.rand(3, patch_size, patch_size)  # np.random.rand()\cf4 \cb1 \strokec4 \
\cb3             \cb1 \
\cb3                 \cf2 \strokec2 # \cf4 \cb1 \strokec4 \
\cb3                 \cf7 \strokec7 outputs\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 clip_model\cf4 \strokec4 (\strokec8 **\cf7 \strokec7 masked_inputs\cf4 \strokec4 ).logits_per_text)\cb1 \
\cb3                 \cf7 \strokec7 result\cf4 \strokec4 [\cf7 \strokec7 row\cf4 \strokec4 ] \strokec8 =\strokec4  \cf7 \strokec7 outputs\cf4 \strokec4 [\cf13 \strokec13 0\cf4 \strokec4 ]\cf9 \strokec9 -\cf7 \strokec7 outputs\cf4 \strokec4 [\cf13 \strokec13 1\cf4 \strokec4 ]\cb1 \
\cb3                 \cf9 \strokec9 print\cf4 \strokec4 (\cf10 \strokec10 f\cf12 \strokec12 "Mask: \cf10 \strokec10 \{\cf7 \strokec7 masked_image_token_ids\cf4 \strokec4 [\cf7 \strokec7 row\cf4 \strokec4 ]\cf10 \strokec10 \}\cf14 \cb3 \strokec14 \\n\cf12 \cb3 \strokec12  Outputs: \cf10 \strokec10 \{\cf7 \strokec7 outputs\cf10 \strokec10 \}\cf12 \strokec12 , difference: \cf10 \strokec10 \{\cf7 \strokec7 result\cf4 \strokec4 [\cf7 \strokec7 row\cf4 \strokec4 ]\cf10 \strokec10 \}\cf12 \strokec12 "\cf4 \strokec4 )\cb1 \
\
\
\
\cb3         \cb1 \
\
\
\cb3     \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 result\cf4 \cb1 \strokec4 \
\
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 get_clip_prediction\cf4 \strokec4 (\cf7 \strokec7 x\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     Shap relevant function. Predict the model output for all combinations of masked tokens.\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     EDUARD: This does the following:\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     - Copy the masked input mapping (paramter x) into text and image parts (originally it's a concatenation)\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     - Use the image of x to part to generate a new image based on the original input variable (global)\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     - Copy x's masked text (input_ids) and the masked image both into masked_inputs variable\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     - Use the masked_inputs variable (consisting of input_ids and pixel_values, both masked) to make a model prediction and get the logit result\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     - Theoretically it works for multiple captions at once, but in practice we don't do it that way.\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     - All this is with x being the OUTPUT of a shapley being done to the X input\cf4 \cb1 \strokec4 \
\cf12 \cb3 \strokec12     """\cf4 \cb1 \strokec4 \
\cb3     \cf5 \strokec5 with\cf4 \strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf6 \strokec6 no_grad\cf4 \strokec4 ():\cb1 \
\cb3         \cf2 \strokec2 # split up the input_ids and the image_token_ids from x (containing both appended)\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 input_ids\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 x\cf4 \strokec4 [:, :\cf7 \strokec7 inputs\cf4 \strokec4 .input_ids.shape[\cf13 \strokec13 1\cf4 \strokec4 ]])\cb1 \
\cb3         \cf7 \strokec7 masked_image_token_ids\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 (\cf7 \strokec7 x\cf4 \strokec4 [:, \cf7 \strokec7 inputs\cf4 \strokec4 .input_ids.shape[\cf13 \strokec13 1\cf4 \strokec4 ]:])\cb1 \
\cb3         \cf2 \strokec2 # select / mask features and normalized boxes from masked_image_token_ids\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 result\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 np\cf4 \strokec4 .\cf9 \strokec9 zeros\cf4 \strokec4 (\cf7 \strokec7 input_ids\cf4 \strokec4 .\cf7 \strokec7 shape\cf4 \strokec4 [\cf13 \strokec13 0\cf4 \strokec4 ])\cb1 \
\cb3         \cf2 \strokec2 # EDUARD: patch_size is a global variable\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 row_cols\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 224\cf4 \strokec4  \strokec8 //\strokec4  \cf7 \strokec7 patch_size\cf4 \strokec4  \cf2 \strokec2 # 224 / 32 = 7\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # call the model for each "new image" generated with masked features\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # EDUARD: While this is a loop, I am fairly certain that in practice only a single example is done, one sentence/image pair (because it's the 0 axis that is being traversed in input_ids not the 1 axis)\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 i\cf4 \strokec4  \cf5 \strokec5 in\cf4 \strokec4  \cf6 \strokec6 range\cf4 \strokec4 (\cf7 \strokec7 input_ids\cf4 \strokec4 .\cf7 \strokec7 shape\cf4 \strokec4 [\cf13 \strokec13 0\cf4 \strokec4 ]):\cb1 \
\cb3             \cf2 \strokec2 # here the actual masking of CLIP is happening. The custom masker only specified which patches to mask, but no actual masking has happened\cf4 \cb1 \strokec4 \
\cb3             \cf2 \strokec2 # EDUARD: The actual image data is downloaded here by making a deepcopy of inputs \cf4 \cb1 \strokec4 \
\cb3             \cf2 \strokec2 # which has all the pixel data for the image being considered ATM.\cf4 \cb1 \strokec4 \
\cb3             \cf7 \strokec7 masked_inputs\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 copy\cf4 \strokec4 .\cf9 \strokec9 deepcopy\cf4 \strokec4 (\cf7 \strokec7 inputs\cf4 \strokec4 )  \cf2 \strokec2 # initialize the thing\cf4 \cb1 \strokec4 \
\cb3             \cf2 \strokec2 # EDUARD: we replace the input ids with the MASKED input ids from the x variable (see first line of the function)\cf4 \cb1 \strokec4 \
\cb3             \cf7 \strokec7 masked_inputs\cf4 \strokec4 [\cf12 \strokec12 'input_ids'\cf4 \strokec4 ] \strokec8 =\strokec4  \cf7 \strokec7 input_ids\cf4 \strokec4 [\cf7 \strokec7 i\cf4 \strokec4 ].\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 )\cb1 \
\cb3             \cf2 \strokec2 # pathify the image\cf4 \cb1 \strokec4 \
\cb3             \cf2 \strokec2 # torch.Size([1, 3, 224, 224]) image size CLIP\cf4 \cb1 \strokec4 \
\cb3             \cf5 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 k\cf4 \strokec4  \cf5 \strokec5 in\cf4 \strokec4  \cf6 \strokec6 range\cf4 \strokec4 (\cf7 \strokec7 masked_image_token_ids\cf4 \strokec4 [\cf7 \strokec7 i\cf4 \strokec4 ].\cf7 \strokec7 shape\cf4 \strokec4 [\cf13 \strokec13 0\cf4 \strokec4 ]):\cb1 \
\cb3                 \cf5 \strokec5 if\cf4 \strokec4  \cf7 \strokec7 masked_image_token_ids\cf4 \strokec4 [\cf7 \strokec7 i\cf4 \strokec4 ][\cf7 \strokec7 k\cf4 \strokec4 ] \cf9 \strokec9 ==\cf4 \strokec4  \cf13 \strokec13 0\cf4 \strokec4 :  \cf2 \strokec2 # should be zero\cf4 \cb1 \strokec4 \
\cb3                     \cf7 \strokec7 m\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 k\cf4 \strokec4  \strokec8 //\strokec4  \cf7 \strokec7 row_cols\cf4 \cb1 \strokec4 \
\cb3                     \cf7 \strokec7 n\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 k\cf4 \strokec4  \strokec8 %\strokec4  \cf7 \strokec7 row_cols\cf4 \cb1 \strokec4 \
\cb3                     \cf7 \strokec7 masked_inputs\cf4 \strokec4 [\cf12 \strokec12 "pixel_values"\cf4 \strokec4 ][:, :, \cf7 \strokec7 m\cf4 \strokec4  \strokec8 *\cb1 \strokec4 \
\cb3                         \cf7 \strokec7 patch_size\cf4 \strokec4 :(\cf7 \strokec7 m\cf4 \strokec8 +\cf13 \strokec13 1\cf4 \strokec4 )\strokec8 *\cf7 \strokec7 patch_size\cf4 \strokec4 , \cf7 \strokec7 n\cf4 \strokec8 *\cf7 \strokec7 patch_size\cf4 \strokec4 :(\cf7 \strokec7 n\cf4 \strokec8 +\cf13 \strokec13 1\cf4 \strokec4 )\strokec8 *\cf7 \strokec7 patch_size\cf4 \strokec4 ] \strokec8 =\strokec4  \cf13 \strokec13 0\cf4 \strokec4  \cf2 \strokec2 # torch.rand(3, patch_size, patch_size)  # np.random.rand()\cf4 \cb1 \strokec4 \
\cb3             \cb1 \
\cb3             \cf2 \strokec2 # \cf4 \cb1 \strokec4 \
\cb3             \cf7 \strokec7 outputs\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 clip_model\cf4 \strokec4 (\strokec8 **\cf7 \strokec7 masked_inputs\cf4 \strokec4 )\cb1 \
\cb3             \cf2 \strokec2 # CLIP does not work with probabilities, because these are computed with softmax among choices (which I do not have here)\cf4 \cb1 \strokec4 \
\cb3             \cf2 \strokec2 # this is the image-text similarity score\cf4 \cb1 \strokec4 \
\cb3             \cf2 \strokec2 # EDUARD: Online docs confirm that logits_per_image is the img-text similarity score\cf4 \cb1 \strokec4 \
\cb3             \cf7 \strokec7 result\cf4 \strokec4 [\cf7 \strokec7 i\cf4 \strokec4 ] \strokec8 =\strokec4  \cf7 \strokec7 outputs\cf4 \strokec4 .logits_per_image\cb1 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 result\cf4 \cb1 \strokec4 \
\
\
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 compute_mm_score\cf4 \strokec4 (\cf7 \strokec7 text_length\cf4 \strokec4 , \cf7 \strokec7 shap_values\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """ Compute Multimodality Score. (80% textual, 20% visual, possibly: 0% knowledge). """\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 text_contrib\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 np\cf4 \strokec4 .\cf7 \strokec7 abs\cf4 \strokec4 (\cf7 \strokec7 shap_values\cf4 \strokec4 .values[\cf13 \strokec13 0\cf4 \strokec4 , \cf13 \strokec13 0\cf4 \strokec4 , :\cf7 \strokec7 text_length\cf4 \strokec4 ]).sum()\cb1 \
\cb3     \cf7 \strokec7 image_contrib\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 np\cf4 \strokec4 .\cf7 \strokec7 abs\cf4 \strokec4 (\cf7 \strokec7 shap_values\cf4 \strokec4 .values[\cf13 \strokec13 0\cf4 \strokec4 , \cf13 \strokec13 0\cf4 \strokec4 , \cf7 \strokec7 text_length\cf4 \strokec4 :]).sum()\cb1 \
\cb3     \cf7 \strokec7 text_score\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 text_contrib\cf4 \strokec4  \strokec8 /\strokec4  (\cf7 \strokec7 text_contrib\cf4 \strokec4  \strokec8 +\strokec4  \cf7 \strokec7 image_contrib\cf4 \strokec4 )\cb1 \
\cb3     \cf2 \strokec2 # image_score = image_contrib / (text_contrib + image_contrib) # is just 1 - text_score in the two modalities case\cf4 \cb1 \strokec4 \
\cb3     \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 text_score\cf4 \cb1 \strokec4 \
\
\
\cf10 \cb3 \strokec10 def\cf4 \strokec4  \cf9 \strokec9 load_models\cf4 \strokec4 (\cf7 \strokec7 which\cf4 \strokec8 =\cf12 \strokec12 "clip"\cf4 \strokec4 ):\cb1 \
\cb3     \cf12 \strokec12 """ Load models and model components. """\cf4 \cb1 \strokec4 \
\
\cb3     \cf5 \strokec5 if\cf4 \strokec4  \cf7 \strokec7 which\cf4 \strokec4  \strokec8 ==\strokec4  \cf12 \strokec12 "clip"\cf4 \strokec4 :\cb1 \
\cb3         \cf7 \strokec7 model\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 CLIPModel\cf4 \strokec4 .\cf9 \strokec9 from_pretrained\cf4 \strokec4 (\cf12 \strokec12 "openai/clip-vit-base-patch32"\cf4 \strokec4 )\cb1 \
\cb3         \cf7 \strokec7 processor\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 CLIPProcessor\cf4 \strokec4 .\cf9 \strokec9 from_pretrained\cf4 \strokec4 (\cf12 \strokec12 "openai/clip-vit-base-patch32"\cf4 \strokec4 )\cb1 \
\cb3         \cf5 \strokec5 return\cf4 \strokec4  \cf7 \strokec7 model\cf4 \strokec4 , \cf7 \strokec7 processor\cf4 \cb1 \strokec4 \
\
\
\cf7 \cb3 \strokec7 clip_model\cf4 \strokec4 , \cf7 \strokec7 clip_processor\cf4 \strokec4  \strokec8 =\strokec4  \cf9 \strokec9 load_models\cf4 \strokec4 (\cf12 \strokec12 "clip"\cf4 \strokec4 )\cb1 \
\
\cf2 \cb3 \strokec2 # load VALSE data as list\cf4 \cb1 \strokec4 \
\cf7 \cb3 \strokec7 data_list\cf4 \strokec4  \strokec8 =\strokec4  \cf9 \strokec9 load_valse_data\cf4 \strokec4 (\cf7 \strokec7 n_samples\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 10\cf4 \strokec4 , \cf7 \strokec7 ling_phenomenon\cf4 \strokec4  \strokec8 =\strokec4  \cf12 \strokec12 "existence"\cf4 \strokec4 )\cb1 \
\
\cf2 \cb3 \strokec2 # we output results of MMSHAP evaluation into a dictionary first, then into a dataframe\cf4 \cb1 \strokec4 \
\cf7 \cb3 \strokec7 results\cf4 \strokec4  \strokec8 =\strokec4  \{ \cf2 \strokec2 # these are already known\cf4 \cb1 \strokec4 \
\cb3             \cf12 \strokec12 "linguistic_phenomena"\cf4 \strokec4 :[],\cb1 \
\cb3            \cf12 \strokec12 "img_path"\cf4 \strokec4 :[],\cb1 \
\cb3            \cf12 \strokec12 "caption"\cf4 \strokec4 :[],\cb1 \
\cb3            \cf12 \strokec12 "foil"\cf4 \strokec4 :[],\cb1 \
\
\cb3            \cf2 \strokec2 # these will be generated:\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "model"\cf4 \strokec4 :[], \cf2 \strokec2 # the type of model being evaluated (CLIP, etc)\cf4 \cb1 \strokec4 \
\
\cb3            \cf12 \strokec12 "model_pred_caption"\cf4 \strokec4 :[], \cf2 \strokec2 #logit for caption\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "model_pred_foil"\cf4 \strokec4 :[], \cf2 \strokec2 #logit for foil\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "correct"\cf4 \strokec4 :[], \cf2 \strokec2 # 1 if correct, 0 if incorrect\cf4 \cb1 \strokec4 \
\
\cb3            \cf12 \strokec12 "mm_score_caption"\cf4 \strokec4 :[], \cf2 \strokec2 # multimodality score for caption-img pair\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "mm_score_foil"\cf4 \strokec4 :[], \cf2 \strokec2 # multimodality score for foil-img pair\cf4 \cb1 \strokec4 \
\
\cb3            \cf12 \strokec12 "shap_values_caption"\cf4 \strokec4 :[], \cf2 \strokec2 # each element is the values attribute of an Explainer object\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "text_tok_caption"\cf4 \strokec4 :[], \cf2 \strokec2 # number of text tokens in caption(so we can see where image tokens start)\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "shap_values_foil"\cf4 \strokec4 :[], \cf2 \strokec2 # foil and caption get separate shapley values\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "text_tok_foil"\cf4 \strokec4 :[], \cf2 \strokec2 # number of text tokens in foil\cf4 \cb1 \strokec4 \
\
\cb3            \cf12 \strokec12 "shap_values_16_caption"\cf4 \strokec4 :[], \cf2 \strokec2 # standardized 16-patch division of images (not depednent on text size),\cf4 \cb1 \strokec4 \
\cb3            \cf12 \strokec12 "shap_values_16_foil"\cf4 \strokec4 :[],\cb1 \
\
\cb3            \}\cb1 \
\
\
\cf5 \cb3 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 data_point\cf4 \strokec4  \cf9 \strokec9 in\cf4 \strokec4  \cf6 \strokec6 tqdm\cf4 \strokec4 (\cf7 \strokec7 data_list\cf4 \strokec4 ):\cb1 \
\
\cb3     \cf2 \strokec2 # copy over those things that should be copied directly\cf4 \cb1 \strokec4 \
\cb3     \cf5 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 name\cf4 \strokec4  \cf5 \strokec5 in\cf4 \strokec4  [\cf12 \strokec12 "img_path"\cf4 \strokec4 , \cf12 \strokec12 "caption"\cf4 \strokec4 , \cf12 \strokec12 "foil"\cf4 \strokec4 , \cf12 \strokec12 "linguistic_phenomena"\cf4 \strokec4 ]:\cb1 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf7 \strokec7 name\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf7 \strokec7 data_point\cf4 \strokec4 [\cf7 \strokec7 name\cf4 \strokec4 ])\cb1 \
\cb3     \cb1 \
\cb3     \cf2 \strokec2 # since we are dealing with CLIP we save the model's name as CLIP\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 results\cf4 \strokec4 [\cf12 \strokec12 "model"\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf12 \strokec12 "CLIP"\cf4 \strokec4 )\cb1 \
\
\cb3     \cf2 \strokec2 # prepare data point as inputs to test model on\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 image\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 Image\cf4 \strokec4 .\cf9 \strokec9 open\cf4 \strokec4 (\cf7 \strokec7 data_point\cf4 \strokec4 [\cf12 \strokec12 "img_path"\cf4 \strokec4 ])\cb1 \
\
\cb3     \cf7 \strokec7 test_sentences\cf4 \strokec4  \strokec8 =\strokec4  [\cf7 \strokec7 data_point\cf4 \strokec4 [\cf12 \strokec12 "caption"\cf4 \strokec4 ],\cb1 \
\cb3                       \cf7 \strokec7 data_point\cf4 \strokec4 [\cf12 \strokec12 "foil"\cf4 \strokec4 ]]\cb1 \
\cb3     \cb1 \
\cb3     \cf2 \strokec2 # create CLIP input with BOTh captions\cf4 \cb1 \strokec4 \
\cb3     \cf5 \strokec5 try\cf4 \strokec4 :\cb1 \
\cb3         \cf7 \strokec7 inputs\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 clip_processor\cf4 \strokec4 (\cb1 \
\cb3             \cf7 \strokec7 text\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 test_sentences\cf4 \strokec4 ,\cb1 \
\cb3             \cf7 \strokec7 images\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 image\cf4 \strokec4 ,\cb1 \
\cb3             \cf7 \strokec7 return_tensors\cf4 \strokec4  \strokec8 =\strokec4  \cf12 \strokec12 'pt'\cf4 \strokec4 ,\cb1 \
\cb3             \cf7 \strokec7 padding\cf4 \strokec4  \strokec8 =\strokec4  \cf10 \strokec10 True\cf4 \cb1 \strokec4 \
\cb3         )\cb1 \
\cb3     \cb1 \
\cb3     \cf5 \strokec5 except\cf4 \strokec4 :\cb1 \
\cb3         \cf9 \strokec9 print\cf4 \strokec4 (\cf10 \strokec10 f\cf12 \strokec12 "Trouble processing \cf10 \strokec10 \{\cf7 \strokec7 data_point\cf4 \strokec4 [\cf12 \strokec12 'img_path'\cf4 \strokec4 ]\cf10 \strokec10 \}\cf12 \strokec12 "\cf4 \strokec4 )\cb1 \
\cb3         \cf5 \strokec5 continue\cf4 \cb1 \strokec4 \
\cb3     \cb1 \
\cb3     \cf7 \strokec7 model_logits\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 clip_model\cf4 \strokec4 (\strokec8 **\cf7 \strokec7 inputs\cf4 \strokec4 ).logits_per_image[\cf13 \strokec13 0\cf4 \strokec4 ,\cf13 \strokec13 0\cf4 \strokec4 :\cf13 \strokec13 2\cf4 \strokec4 ].tolist()\cb1 \
\
\cb3     \cf2 \strokec2 #SHAP part\cf4 \cb1 \strokec4 \
\cb3     \cf11 \cb3 \strokec11 NR_ROWS\cf4 \cb3 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 4\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 patch_size\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 224\cf4 \strokec4  \strokec8 //\strokec4  \cf11 \cb3 \strokec11 NR_ROWS\cf4 \cb3 \strokec4  \cf2 \strokec2 # determine patch nr and patch size based on nr of tokens assuming 224x224 image\cf4 \cb1 \strokec4 \
\cb3     \cf7 \strokec7 image_token_ids\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 ( \cb1 \
\cb3             \cf6 \strokec6 range\cf4 \strokec4 (\cf13 \strokec13 1\cf4 \strokec4 , \cf11 \cb3 \strokec11 NR_ROWS\cf4 \cb3 \strokec8 **\cf13 \strokec13 2\cf4 \strokec8 +\cf13 \strokec13 1\cf4 \strokec4 )).\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 ) \cf2 \strokec2 #p x p patches        \cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 # \cf10 \strokec10 NOTE\cf2 \strokec2 : There is no image data in X;The actual image pixel data is accessed by get_clip_prediction() from the inputs variable\cf4 \cb1 \strokec4 \
\cb3     \cf11 \cb3 \strokec11 X\cf4 \cb3 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 image_token_ids\cf4 \strokec4 .\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 1\cf4 \strokec4 ) \cf2 \strokec2 # only image token ids are passed to the explainer. Text is added by the customer_masked_image_only function\cf4 \cb1 \strokec4 \
\
\cb3     \cf7 \strokec7 explainer\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 shap\cf4 \strokec4 .\cf6 \strokec6 Explainer\cf4 \strokec4 (\cb1 \
\cb3             \cf9 \strokec9 get_clip_prediction_diff\cf4 \strokec4 , \cf9 \strokec9 custom_masker_image_only_no_text_output\cf4 \strokec4 , \cf7 \strokec7 silent\cf4 \strokec8 =\cf10 \strokec10 False\cf4 \strokec4 )\cb1 \
\cb3     \cf7 \strokec7 shap_values_16\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 explainer\cf4 \strokec4 (\cf11 \cb3 \strokec11 X\cf4 \cb3 \strokec4 )\cb1 \
\
\
\
\
\cb3     \cf5 \strokec5 pass\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 # run each sentence-image pair (caption-image and foil-image) through model separately\cf4 \cb1 \strokec4 \
\cb3     \cf5 \strokec5 for\cf4 \strokec4  \cf7 \strokec7 k\cf4 \strokec4 , \cf7 \strokec7 sentence\cf4 \strokec4  \cf5 \strokec5 in\cf4 \strokec4  \cf6 \strokec6 enumerate\cf4 \strokec4 (\cf7 \strokec7 test_sentences\cf4 \strokec4 ):\cb1 \
\
\cb3         \cf2 \strokec2 # first generate image-text combination with unmasked image\cf4 \cb1 \strokec4 \
\
\cb3         \cf5 \strokec5 try\cf4 \strokec4 :  \cf2 \strokec2 # image feature extraction can go wrong\cf4 \cb1 \strokec4 \
\cb3             \cf7 \strokec7 inputs\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 clip_processor\cf4 \strokec4 (\cb1 \
\cb3                 \cf7 \strokec7 text\cf4 \strokec8 =\cf7 \strokec7 sentence\cf4 \strokec4 , \cf7 \strokec7 images\cf4 \strokec8 =\cf7 \strokec7 image\cf4 \strokec4 , \cf7 \strokec7 return_tensors\cf4 \strokec8 =\cf12 \strokec12 "pt"\cf4 \strokec4 , \cf7 \strokec7 padding\cf4 \strokec8 =\cf10 \strokec10 True\cf4 \cb1 \strokec4 \
\cb3             )\cb1 \
\cb3         \cf5 \strokec5 except\cf4 \strokec4 :\cb1 \
\cb3             \cf9 \strokec9 print\cf4 \strokec4 (\cf10 \strokec10 f\cf12 \strokec12 "Trouble processing \cf10 \strokec10 \{\cf7 \strokec7 data_point\cf4 \strokec4 [\cf12 \strokec12 'img_path'\cf4 \strokec4 ]\cf10 \strokec10 \}\cf12 \strokec12 "\cf4 \strokec4 )\cb1 \
\cb3             \cf5 \strokec5 continue\cf4 \cb1 \strokec4 \
\
\cb3         \cf2 \strokec2 # Get prediction as logit for this text-img combo (it is 0x0 because only one image and caption was given)\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # this is the similarity score between the caption or foil and the UNMASKED image :)\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 model_prediction\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 clip_model\cf4 \strokec4 (\strokec8 **\cf7 \strokec7 inputs\cf4 \strokec4 ).logits_per_image[\cf13 \strokec13 0\cf4 \strokec4 ,\cf13 \strokec13 0\cf4 \strokec4 ].item()\cb1 \
\
\cb3         \cf2 \strokec2 # SHAP part:\cf4 \cb1 \strokec4 \
\
\
\cb3         \cf2 \strokec2 # we want roughly equal tokens & patches (p x p = tokens) \cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # [\cf10 \strokec10 NOTE\cf2 \strokec2 : later could run a different version with MORE patches for granular visual]\cf4 \cb1 \strokec4 \
\cb3         \cb1 \
\cb3         \cf2 \strokec2 # input_ds.input_ds is shaped like this: [nr texts, nr tokens per text]. In this case it will be [1, n_tokens]\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 text_length_tok\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 inputs\cf4 \strokec4 .input_ids.shape[\cf13 \strokec13 1\cf4 \strokec4 ] \cf2 \strokec2 # nr of text tokens\cf4 \cb1 \strokec4 \
\
\cb3         \cf2 \strokec2 # 1) Roughly equal text and image tokens (for multimodality score) -- explainer1\cf4 \cb1 \strokec4 \
\
\cb3         \cf7 \strokec7 p\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 int\cf4 \strokec4 (\cf6 \strokec6 math\cf4 \strokec4 .\cf9 \strokec9 ceil\cf4 \strokec4 (\cf6 \strokec6 np\cf4 \strokec4 .\cf7 \strokec7 sqrt\cf4 \strokec4 (\cf7 \strokec7 text_length_tok\cf4 \strokec4 )))\cb1 \
\cb3         \cf7 \strokec7 patch_size\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 224\cf4 \strokec4  \strokec8 //\strokec4  \cf7 \strokec7 p\cf4 \strokec4  \cf2 \strokec2 # determine patch nr and patch size based on nr of tokens assuming 224x224 image\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 image_token_ids\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 ( \cb1 \
\cb3             \cf6 \strokec6 range\cf4 \strokec4 (\cf13 \strokec13 1\cf4 \strokec4 , \cf7 \strokec7 p\cf4 \strokec8 **\cf13 \strokec13 2\cf4 \strokec8 +\cf13 \strokec13 1\cf4 \strokec4 )).\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 ) \cf2 \strokec2 #p x p patches        \cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # \cf10 \strokec10 NOTE\cf2 \strokec2 : There is no image data in X;The actual image pixel data is accessed by get_clip_prediction() from the inputs variable\cf4 \cb1 \strokec4 \
\cb3         \cf11 \cb3 \strokec11 X1\cf4 \cb3 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 cat\cf4 \strokec4 (\cb1 \
\cb3             (\cf7 \strokec7 inputs\cf4 \strokec4 .input_ids, \cf7 \strokec7 image_token_ids\cf4 \strokec4 ), \cf13 \strokec13 1\cf4 \strokec4 ).\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 1\cf4 \strokec4 )\cb1 \
\cb3         \cf9 \strokec9 print\cf4 \strokec4 (\cf7 \strokec7 inputs\cf4 \strokec4 )\cb1 \
\
\cb3         \cf2 \strokec2 # create an explainer with model and image masker\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 explainer1\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 shap\cf4 \strokec4 .\cf6 \strokec6 Explainer\cf4 \strokec4 (\cb1 \
\cb3             \cf9 \strokec9 get_clip_prediction\cf4 \strokec4 , \cf9 \strokec9 custom_masker_bimodal\cf4 \strokec4 , \cf7 \strokec7 silent\cf4 \strokec8 =\cf10 \strokec10 False\cf4 \strokec4 )\cb1 \
\cb3         \cf7 \strokec7 shap_values\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 explainer1\cf4 \strokec4 (\cf11 \cb3 \strokec11 X1\cf4 \cb3 \strokec4 )\cb1 \
\
\cb3         \cf2 \strokec2 # 2) STANDARDIZED number of image tokens (16) -- explainer2\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # ONLY THE IMAGE IS MASKED HERE, text is left untouched\cf4 \cb1 \strokec4 \
\
\cb3         \cf11 \cb3 \strokec11 NR_ROWS\cf4 \cb3 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 4\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 patch_size\cf4 \strokec4  \strokec8 =\strokec4  \cf13 \strokec13 224\cf4 \strokec4  \strokec8 //\strokec4  \cf11 \cb3 \strokec11 NR_ROWS\cf4 \cb3 \strokec4  \cf2 \strokec2 # determine patch nr and patch size based on nr of tokens assuming 224x224 image\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 image_token_ids\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 torch\cf4 \strokec4 .\cf9 \strokec9 tensor\cf4 \strokec4 ( \cb1 \
\cb3             \cf6 \strokec6 range\cf4 \strokec4 (\cf13 \strokec13 1\cf4 \strokec4 , \cf11 \cb3 \strokec11 NR_ROWS\cf4 \cb3 \strokec8 **\cf13 \strokec13 2\cf4 \strokec8 +\cf13 \strokec13 1\cf4 \strokec4 )).\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 ) \cf2 \strokec2 #p x p patches        \cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # \cf10 \strokec10 NOTE\cf2 \strokec2 : There is no image data in X;The actual image pixel data is accessed by get_clip_prediction() from the inputs variable\cf4 \cb1 \strokec4 \
\cb3         \cf11 \cb3 \strokec11 X2\cf4 \cb3 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 image_token_ids\cf4 \strokec4 .\cf9 \strokec9 unsqueeze\cf4 \strokec4 (\cf13 \strokec13 1\cf4 \strokec4 ) \cf2 \strokec2 # only image token ids are passed to the explainer. Text is added by the customer_masked_image_only function\cf4 \cb1 \strokec4 \
\
\cb3         \cf7 \strokec7 explainer2\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 shap\cf4 \strokec4 .\cf6 \strokec6 Explainer\cf4 \strokec4 (\cb1 \
\cb3             \cf9 \strokec9 get_clip_prediction\cf4 \strokec4 , \cf9 \strokec9 custom_masker_image_only\cf4 \strokec4 , \cf7 \strokec7 silent\cf4 \strokec8 =\cf10 \strokec10 False\cf4 \strokec4 )\cb1 \
\cb3         \cf7 \strokec7 shap_values_16\cf4 \strokec4  \strokec8 =\strokec4  \cf7 \strokec7 explainer2\cf4 \strokec4 (\cf11 \cb3 \strokec11 X2\cf4 \cb3 \strokec4 )\cb1 \
\
\
\cb3         \cf2 \strokec2 # assign a score to each region and each word as well\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # Seems like words and regions are being treated as basically equivalent. We need them to have an equal number (roughly...) so that the score can be comparable\cf4 \cb1 \strokec4 \
\cb3     \cb1 \
\cb3         \cf2 \strokec2 # EDUARD: This just sums the Shapley values of the text tokens vs the image patches. Whoever gets a bigger sum is more important. That's it!\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 mm_score\cf4 \strokec4  \strokec8 =\strokec4  \cf9 \strokec9 compute_mm_score\cf4 \strokec4 (\cf7 \strokec7 text_length_tok\cf4 \strokec4 , \cf7 \strokec7 shap_values\cf4 \strokec4 )\cb1 \
\
\cb3         \cf2 \strokec2 # if it's CAPTION (so k=0), update the data for caption        \cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 if\cf4 \strokec4  \cf7 \strokec7 k\cf4 \strokec4  \strokec8 ==\strokec4  \cf13 \strokec13 0\cf4 \strokec4 :\cb1 \
\cb3             \cf7 \strokec7 which\cf4 \strokec4  \strokec8 =\strokec4  \cf12 \strokec12 'caption'\cf4 \cb1 \strokec4 \
\cb3         \cf2 \strokec2 # if it's FOIL (so k=1), update the data for foil\cf4 \cb1 \strokec4 \
\cb3         \cf5 \strokec5 else\cf4 \strokec4 :\cb1 \
\cb3             \cf7 \strokec7 which\cf4 \strokec4  \strokec8 =\strokec4  \cf12 \strokec12 'foil'\cf4 \cb1 \strokec4 \
\
\cb3         \cf2 \strokec2 # save relevant fields to the dictionary to either \{caption\} or \{foil\} (based on which it is)\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf12 \strokec12 "model_pred_"\cf4 \strokec8 +\cf7 \strokec7 which\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf7 \strokec7 model_prediction\cf4 \strokec4 ) \cf2 \strokec2 # logit\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf10 \strokec10 f\cf12 \strokec12 "mm_score_"\cf4 \strokec8 +\cf7 \strokec7 which\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf7 \strokec7 mm_score\cf4 \strokec4 ) \cf2 \strokec2 #multimodality score\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf10 \strokec10 f\cf12 \strokec12 "shap_values_"\cf4 \strokec8 +\cf7 \strokec7 which\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf7 \strokec7 shap_values\cf4 \strokec4 .values) \cf2 \strokec2 # shap values array\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf10 \strokec10 f\cf12 \strokec12 "shap_values_16_"\cf4 \strokec8 +\cf7 \strokec7 which\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf7 \strokec7 shap_values_16\cf4 \strokec4 .values) \cf2 \strokec2 # shap values with 16 tokens (standard)\cf4 \cb1 \strokec4 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf10 \strokec10 f\cf12 \strokec12 "text_tok_"\cf4 \strokec8 +\cf7 \strokec7 which\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf7 \strokec7 text_length_tok\cf4 \strokec4 ) \cf2 \strokec2 # number of text tokens\cf4 \cb1 \strokec4 \
\
\
\
\cb3     \cf2 \strokec2 # print(results)\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 # having predicted both caption and foil, save whether it's correct or not    \cf4 \cb1 \strokec4 \
\cb3     \cf5 \strokec5 if\cf4 \strokec4  \cf7 \strokec7 results\cf4 \strokec4 [\cf12 \strokec12 "model_pred_caption"\cf4 \strokec4 ][\strokec8 -\cf13 \strokec13 1\cf4 \strokec4 ] \strokec8 >\strokec4  \cf7 \strokec7 results\cf4 \strokec4 [\cf12 \strokec12 "model_pred_foil"\cf4 \strokec4 ][\strokec8 -\cf13 \strokec13 1\cf4 \strokec4 ]:\cb1 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf12 \strokec12 "correct"\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf13 \strokec13 1\cf4 \strokec4 )\cb1 \
\cb3     \cf5 \strokec5 else\cf4 \strokec4 :\cb1 \
\cb3         \cf7 \strokec7 results\cf4 \strokec4 [\cf12 \strokec12 "correct"\cf4 \strokec4 ].\cf9 \strokec9 append\cf4 \strokec4 (\cf13 \strokec13 0\cf4 \strokec4 )\cb1 \
\
\
\cf2 \cb3 \strokec2 #     for what, mm_scores in results["text_score"].items():\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #         if len(mm_scores) > 0:\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #             print(\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #                 f"""We tested CLIP on \{len(results)\} samples.\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #     The MM_score is: \{np.array(mm_scores).mean()*100:.2f\}% +/- \{np.array(mm_scores).std()*100:.2f\}% textual, the rest visual.""")\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 #     print(f"""The pairwise_accuracy is: \{np.array(results["acc_r"]).mean()*100:.2f\}%.\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 # ------""")\cf4 \cb1 \strokec4 \
\
\cb3     \cf2 \strokec2 # # writing results down to a json file for further analysis of results on VALSE\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 # if write_res == 'yes':\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 #     path = f"result_jsons/clip_\{num_samples\}/"\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 #     os.makedirs(path, exist_ok=True)\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 #     with open(f'result_jsons/clip_\{num_samples\}/\{instrument\}.json', 'w') as f:\cf4 \cb1 \strokec4 \
\cb3     \cf2 \strokec2 #         json.dump(foils_data, f)\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 results_df\cf4 \strokec4  \strokec8 =\strokec4  \cf6 \strokec6 pd\cf4 \strokec4 .\cf6 \strokec6 DataFrame\cf4 \strokec4 (\cf7 \strokec7 results\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 results_df\cf4 \strokec4 .\cf9 \strokec9 to_pickle\cf4 \strokec4 (\cf12 \strokec12 "../data_analysis/clip_mmshap_results.pickle"\cf4 \strokec4 )\cb1 \
}